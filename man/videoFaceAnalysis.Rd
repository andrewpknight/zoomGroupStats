% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/videoFaceAnalysis.R
\name{videoFaceAnalysis}
\alias{videoFaceAnalysis}
\title{Analyze the facial features within an exported Zoom video file}
\usage{
videoFaceAnalysis(
  inputVideo,
  recordingStartDateTime,
  sampleWindow,
  facesCollectionID = NA,
  videoImageDirectory = NULL
)
}
\arguments{
\item{inputVideo}{string path to the video file (ideal is gallery)}

\item{recordingStartDateTime}{YYYY-MM-DD HH:MM:SS of the start of the recording}

\item{sampleWindow}{Frame rate for the analysis}

\item{facesCollectionID}{name of an AWS collection with identified faces}

\item{videoImageDirectory}{path to a directory containing pre-split images. This will skip the splitting of an image file, but it presumes that images were pre-split using grabVideoStills}
}
\value{
data.frame with one record for every face detected in each frame. For each face, there is an abundance of information from AWS rekognition. This output is quite detailed. Note that there will be a varying number of faces per sampled frame in the video. Imagine that you have sampled the meeting and had someone rate each person's face within that sampled moment.
}
\description{
Analyze the facial features within an exported Zoom video file
}
\examples{
\dontrun{
vid.out = videoFaceAnalysis(inputVideo="sample_gallery_video.mp4", 
recordingStartDateTime="2020-04-20 13:30:00", 
sampleWindow=30, facesCollectionID="group-r")
}
}
